{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ƒ Solution for Exercise 04\n",
    "\n",
    "In the previous notebook, we illustrated how the regularization\n",
    "parameter of the Ridge model needs to be optimized by hand.\n",
    "However, this way of optimizing hyperparameters is not effective:\n",
    "we did a single split, while cross-validation would be\n",
    "much more effective.\n",
    "\n",
    "This exercise will make you implement the same search but using the class\n",
    "`GridSearchCV`.\n",
    "\n",
    "First, we will:\n",
    "\n",
    "* load the California housing dataset;\n",
    "* split the data into a training and testing set;\n",
    "* create a machine learning pipeline composed of a standard scaler to\n",
    "  normalize the data, and a ridge regression as a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the exercise is to use a `GridSearchCV` estimator to optimize the\n",
    "parameter `alpha` of the ridge regressor. Let's redefine the\n",
    "regularization parameter candidates like we did in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-1, 2, num=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refer to the documentation of `GridSearchCV`\n",
    "[here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "to check how to use this estimator.\n",
    "First, train the grid-search using the previous machine learning pipeline and\n",
    "print the value of the best parameter found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to check the name of the parameter to tune in the pipeline.\n",
    "We can use `get_params()` to know more about the available parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in ridge.get_params().keys():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "search = GridSearchCV(ridge, param_grid={\"ridge__alpha\": alphas})\n",
    "search.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"Best alpha found via the grid-search:\\n\"\n",
    "    f\"{search.best_params_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you found the best parameter `alpha`, use the grid-search estimator\n",
    "that you created to predict and estimate the $R^2$ score of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R2 score of the optimal ridge is:\\n\"\n",
    "    f\"{search.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also interesting to know that several regressors and classifiers\n",
    "in scikit-learn are optimized to make this parameter tuning. They usually\n",
    "finish with the term \"CV\" for \"Cross Validation\" (e.g. `RidgeCV`).\n",
    "They are more efficient than using `GridSearchCV` and you should use them\n",
    "instead.\n",
    "\n",
    "Repeat the previous exercise using the `RidgeVC` estimator\n",
    "instead of a grid-search.\n",
    "Refer to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "of `RidgeCV` for more information on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), RidgeCV(alphas=alphas))\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\n",
    "    f\"Best alpha found via the grid-search:\\n\"\n",
    "    f\"{ridge[-1].alpha_}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"R2 score of the optimal ridge is:\\n\"\n",
    "    f\"{ridge.score(X_test, y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
