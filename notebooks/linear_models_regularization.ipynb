{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization of linear regression model\n",
    "\n",
    "In this notebook, we will see the limitations of linear regression models and\n",
    "the advantage of using regularized models instead.\n",
    "\n",
    "Besides, we will also present the preprocessing required when dealing\n",
    "with regularized models, furthermore when the regularization parameter\n",
    "needs to be fine-tuned.\n",
    "\n",
    "We will start by highlighting the over-fitting issue that can arise with\n",
    "a simple linear regression model.\n",
    "\n",
    "## Effect of regularization\n",
    "\n",
    "We will first load the California housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous exercise, we will use an independent test set to evaluate\n",
    "the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one of the previous notebook, we showed that linear models could be used\n",
    "even in settings where `X` and `y` are not linearly linked.\n",
    "\n",
    "We showed that one can use the `PolynomialFeatures` transformer to create\n",
    "additional features encoding non-linear interactions between features.\n",
    "\n",
    "Here, we will use this transformer to augment the feature space.\n",
    "Subsequently, we will train a linear regression model.\n",
    "We will use the out-of-sample test set to evaluate the\n",
    "generalization capabilities of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                                  LinearRegression())\n",
    "linear_regression.fit(X_train, y_train)\n",
    "test_score = linear_regression.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of linear regresion model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we obtain an $R^2$ score below zero.\n",
    "\n",
    "It means that our model is far worth than predicting the mean of `y_train`.\n",
    "This issue is due to overfitting.\n",
    "We can compute the score on the training set to confirm this intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = linear_regression.score(X_train, y_train)\n",
    "print(f\"R2 score of linear regresion model on the train set:\\n\"\n",
    "      f\"{train_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score on the training set is much better. This performance gap between\n",
    "the training and testing score is an indication that our model overfitted\n",
    "our training set.\n",
    "\n",
    "Indeed, this is one of the danger when augmenting the number of features\n",
    "with a `PolynomialFeatures` transformer. Our model will focus on some\n",
    "specific features. We can check the weights of the model to have a\n",
    "confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "weights_linear_regression = pd.Series(\n",
    "    linear_regression[-1].coef_,\n",
    "    index=linear_regression[0].get_feature_names(input_features=X.columns))\n",
    "_, ax = plt.subplots(figsize=(6, 16))\n",
    "_ = weights_linear_regression.plot(kind=\"barh\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force the linear regression model to consider all features in a more\n",
    "homogeneous manner. In fact, we could force large positive or negative weight\n",
    "to shrink toward zero. This is known as regularization. We will use a ridge\n",
    "model which enforces such behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = make_pipeline(PolynomialFeatures(degree=2),\n",
    "                      Ridge(alpha=0.5))\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = ridge.score(X_train, y_train)\n",
    "print(f\"R2 score of ridge model on the train set:\\n\"\n",
    "      f\"{train_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = ridge.score(X_test, y_test)\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training and testing scores are much closer, indicating that\n",
    "our model is less overfitting. We can compare the values of the weights of\n",
    "ridge with the un-regularized linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ridge = pd.Series(\n",
    "    ridge[-1].coef_,\n",
    "    index=ridge[0].get_feature_names(input_features=X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.concat(\n",
    "    [weights_linear_regression, weights_ridge], axis=1,\n",
    "    keys=[\"Linear regression\", \"Ridge\"])\n",
    "\n",
    "_, ax = plt.subplots(figsize=(6, 16))\n",
    "weights.plot(kind=\"barh\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the magnitude of the weights are shrunk towards zero in\n",
    "comparison with the linear regression model.\n",
    "\n",
    "However, in this example, we omitted two important aspects: (i) the need to\n",
    "scale the data and (ii) the need to search for the best regularization\n",
    "parameter.\n",
    "\n",
    "## Scale your data!\n",
    "\n",
    "Regularization will add constraints on weights of the model.\n",
    "We saw in the previous example that a ridge model will enforce\n",
    "that all weights have a similar magnitude.\n",
    "\n",
    "Indeed, the larger alpha is, the larger this enforcement will be.\n",
    "\n",
    "This procedure should make us think about feature rescaling.\n",
    "Let's consider the case where features have an identical data dispersion:\n",
    "if two features are found equally important by the model, they will be\n",
    "affected close weights in term of norm.\n",
    "\n",
    "Now, let's consider the scenario where features have completely different\n",
    "data dispersion (e.g. age in years and annual revenue in dollars).\n",
    "If two features are as important, our model will boost the weights of\n",
    "features with small dispersion and reduce the weights of features with\n",
    "high dispersion.\n",
    "\n",
    "We recall that regularization forces weights to be closer.\n",
    "\n",
    "Therefore, we get an intuition that if we want to use regularization, dealing\n",
    "with rescaled data would make it easier to find an optimal regularization\n",
    "parameter and thus an adequate model.\n",
    "\n",
    "As a side note, some solvers based on gradient # computation are expecting\n",
    "such rescaled data.\n",
    "Unscaled data will be detrimental when computing the optimal weights.\n",
    "\n",
    "Therefore, when working with a linear model and numerical data,\n",
    "it is generally good practice to scale the data.\n",
    "\n",
    "In the remaining of this section, we will present the basics on how to\n",
    "incorporate a scaler within your machine learning pipeline.\n",
    "\n",
    "Scikit-learn provides several tools to preprocess the data, such as\n",
    "the `StandardScaler`, which transforms the data in order for each feature\n",
    "to have a mean of zero and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit(X_train).transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scikit-learn estimator is known as a transformer: it computes some\n",
    "statistics (i.e the mean and the standard deviation) and stores them as\n",
    "attributes (`scaler.mean_`, `scaler.scale_`) when calling `fit`. Using these\n",
    "stats, it transforms the data when `transform` is called. Therefore, it\n",
    "is important to note that `fit` should only be called on the training data,\n",
    "similar to classifiers and regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean records on the training set:\\n', scaler.mean_)\n",
    "print('standard deviation records on the training set:\\n', scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, `X_train_scaled` is the data scaled, using the\n",
    "mean and standard deviation of each feature, computed using the training\n",
    "data `X_train`.\n",
    "\n",
    "Thus, we can use these scaled dataset to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train_scaled, y_train)\n",
    "test_score = ridge.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calling the transformer to transform the data and then calling the\n",
    "regressor, scikit-learn provides a `Pipeline`, which 'chains' the transformer\n",
    "and regressor together. The pipeline allows you to use a sequence of\n",
    "transformer(s) followed by a regressor or a classifier, in one call. (i.e.\n",
    "fitting the pipeline will fit both the transformer(s) and the regressor. Then\n",
    "predicting from the pipeline will first transform the data through the\n",
    "transformer(s) then predict with the regressor from the transformed data)\n",
    "\n",
    "This pipeline exposes the same API as the regressor and classifier and will\n",
    "manage the calls to `fit` and `transform` for you, avoiding any problems with\n",
    "data leakage (when knowledge of the test data was inadvertently included in\n",
    "training a model, as when fitting a transformer on the test data).\n",
    "\n",
    "We already used `Pipeline` to create the polynomial features before training\n",
    "the model.\n",
    "\n",
    "We will can create a new one by using `make_pipeline` and giving as\n",
    "arguments the transformation(s) to be performed (in order) and the regressor\n",
    "model.\n",
    "\n",
    "Here, we can implement the scaling process before training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      Ridge(alpha=0.5))\n",
    "ridge.fit(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in this example, using a pipeline simplifies the manual handling.\n",
    "\n",
    "When creating the model, keeping the same `alpha` does not give good results.\n",
    "It depends on the data provided. Therefore, it needs to be tuned for each\n",
    "dataset.\n",
    "\n",
    "In the next section, we will present the steps to tune this parameter.\n",
    "\n",
    "## Fine tuning the regularization parameter\n",
    "\n",
    "As mentioned, the regularization parameter needs to be tuned on each dataset.\n",
    "The default parameter will not lead to the optimal model. Therefore, we need\n",
    "to tune the `alpha` parameter.\n",
    "\n",
    "Model hyperparameters tuning should be done with care. Indeed, we want to find\n",
    "an optimal parameter that maximizes some metrics.\n",
    "Thus, it requires both a training set and testing set.\n",
    "\n",
    "However, this testing set should be different from the out-of-sample testing set\n",
    "that we used to evaluate our model:\n",
    "if we use the same one, we are using an `alpha` which was optimized for\n",
    "this testing set and it breaks the out-of-sample rule.\n",
    "\n",
    "Therefore, we can split our previous training set into two subsets: a\n",
    "new training set and a validation set which we will use later to pick\n",
    "the optimal value for `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub_train, X_valid, y_sub_train, y_valid = train_test_split(\n",
    "    X_train, y_train, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.logspace(-10, -1, num=30)\n",
    "list_ridge_scores = []\n",
    "for alpha in alphas:\n",
    "    ridge.set_params(ridge__alpha=alpha)\n",
    "    ridge.fit(X_sub_train, y_sub_train)\n",
    "    list_ridge_scores.append(ridge.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, list_ridge_scores, \"+-\", label='Ridge')\n",
    "plt.xlabel('alpha (regularization strength)')\n",
    "plt.ylabel('R2 score (higher is better)')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, regularization is just like salt in cooking:\n",
    "one must balance its amount to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = alphas[np.argmax(list_ridge_scores)]\n",
    "best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can re-train a Ridge model on the full dataset,\n",
    "with the best value for alpha we found earlier, and check the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),\n",
    "                      Ridge(alpha=best_alpha))\n",
    "ridge.fit(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "\n",
    "print(f\"R2 score of ridge model on the test set:\\n\"\n",
    "      f\"{test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next exercise, you will use a scikit-learn estimator which allows to\n",
    "make some parameters tuning instead of programming yourself a `for` loop by\n",
    "hand.\n",
    "\n",
    "In this notebook, you learned about the concept of regularization and\n",
    "the importance of preprocessing and parameter tuning."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
