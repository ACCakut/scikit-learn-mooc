{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit-generalization-underfit\n",
    "\n",
    "In the previous notebook, we presented the general cross-validation framework\n",
    "and it helps at quantifying the empirical and generalization errors as well\n",
    "as their fluctuations.\n",
    "\n",
    "In this notebook, we will put these two errors into perspective and show how\n",
    "they can help us know if our model generalizes, overfit, or underfit.\n",
    "\n",
    "Let's first load the data and create the identical model as in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X, y = housing.data, housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "regressor = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs. underfitting\n",
    "\n",
    "To better understand the performance of our model and maybe find insights on\n",
    "how to improve it we will compare the generalization error with the empirical\n",
    "error. Thus, we need to compute the error on the training set, which is\n",
    "possible using the `cross_validate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate, ShuffleSplit\n",
    "\n",
    "cv = ShuffleSplit(n_splits=30, test_size=0.2)\n",
    "cv_results = cross_validate(regressor, X, y,\n",
    "                            cv=cv, scoring=\"neg_mean_absolute_error\",\n",
    "                            return_train_score=True, n_jobs=2)\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select the train and test score and take the error instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "scores[[\"train error\", \"test error\"]] = -cv_results[\n",
    "    [\"train_score\", \"test_score\"]]\n",
    "sns.histplot(scores, bins=50)\n",
    "_ = plt.xlabel(\"Mean absolute error (k$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the distribution of the empirical and generalization errors, we\n",
    "get information about whether our model is over-fitting, under-fitting (or\n",
    "both at the same time).\n",
    "\n",
    "Here, we observe a **small empirical error** (actually zero), meaning that\n",
    "the model is **not under-fitting**: it is flexible enough to capture any\n",
    "variations present in the training set.\n",
    "\n",
    "However the **significantly larger generalization error** tells us that the\n",
    "model is **over-fitting**: the model has memorized many variations of the\n",
    "training set that could be considered \"noisy\" because they do not generalize\n",
    "to help us make good prediction on the test set.\n",
    "\n",
    "## Validation curve\n",
    "\n",
    "Some model hyperparameters are usually the key to go from a model that\n",
    "underfits to a model that overfits, hopefully going through a region were we\n",
    "can get a good balance between the two. We can acquire knowledge by plotting\n",
    "a curve called the validation curve. This curve applies the above experiment\n",
    "and varies the value of a hyperparameter.\n",
    "\n",
    "For the decision tree, the `max_depth` the main parameter to control the\n",
    "trade-off between under-fitting and over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "max_depth = [1, 5, 10, 15, 20, 25]\n",
    "train_scores, test_scores = validation_curve(\n",
    "    regressor, X, y, param_name=\"max_depth\", param_range=max_depth,\n",
    "    cv=cv, scoring=\"neg_mean_absolute_error\", n_jobs=2)\n",
    "train_errors, test_errors = -train_scores, -test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we collected the results, we will show the validation curve by\n",
    "plotting the empirical and generalization errors (as well as their\n",
    "deviations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "\n",
    "error_type = [\"Empirical error\", \"Generalization error\"]\n",
    "errors = [train_errors, test_errors]\n",
    "\n",
    "for name, err in zip(error_type, errors):\n",
    "    ax.plot(max_depth, err.mean(axis=1), linestyle=\"-.\", label=name,\n",
    "            alpha=0.8)\n",
    "    ax.fill_between(max_depth, err.mean(axis=1) - err.std(axis=1),\n",
    "                    err.mean(axis=1) + err.std(axis=1), alpha=0.5,\n",
    "                    label=f\"std. dev. {name.lower()}\")\n",
    "\n",
    "ax.set_xticks(max_depth)\n",
    "ax.set_xlabel(\"Maximum depth of decision tree\")\n",
    "ax.set_ylabel(\"Mean absolute error (k$)\")\n",
    "ax.set_title(\"Validation curve for decision tree\")\n",
    "_ = plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation curve can be divided into three areas:\n",
    "\n",
    "- For `max_depth < 10`, the decision tree underfits. The empirical error and\n",
    "  therefore also the generalization error are both high. The model is too\n",
    "  constrained and cannot capture much of the variability of the target\n",
    "  variable.\n",
    "\n",
    "- The region around `max_depth = 10` corresponds to the parameter for which\n",
    "  the decision tree generalizes the best. It is flexible enough to capture a\n",
    "  fraction of the variability of the target that generalizes, while not\n",
    "  memorizing all of the noise in the target.\n",
    "\n",
    "- For `max_depth > 10`, the decision tree overfits. The empirical error\n",
    "  becomes very small, while the generalization error increases. In this\n",
    "  region, the models captures too much of the noisy part of the variations of\n",
    "  the target and this harms its ability to generalize well to test data.\n",
    "\n",
    "Note that for `max_depth = 10`, the model overfits a bit as there is a gap\n",
    "between the empirical error and the generalization error. It can also\n",
    "potentially underfit also a bit at the same time, because the empirical error\n",
    "is still far from zero (more than 30 k\\$), meaning that the model might\n",
    "still be too constrained to model interesting parts of the data. However the\n",
    "generalization error is minimal, and this is what really matters. This is the\n",
    "best compromise we could reach by just tuning this parameter.\n",
    "\n",
    "We were lucky that the variance of the errors was small compared to their\n",
    "respective values, and therefore the conclusions above are quite clear. This\n",
    "is not necessarily always the case.\n",
    "\n",
    "## Summary:\n",
    "\n",
    "In this notebook, we saw:\n",
    "\n",
    "* how to identify if a model is generalizing, overfitting, or underfitting;\n",
    "* how to check influence of an hyperparameter on the trade-off\n",
    "  underfit/overfit."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "percent"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
