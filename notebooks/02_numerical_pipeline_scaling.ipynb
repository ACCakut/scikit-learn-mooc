{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for numerical features\n",
    "\n",
    "In this notebook, we present how to build predictive models on tabular\n",
    "datasets, with only numerical features.\n",
    "\n",
    "In particular we will highlight:\n",
    "\n",
    "* an example of preprocessing, namely the **scaling numerical variables**;\n",
    "* using a scikit-learn **pipeline** to chain preprocessing and model\n",
    "  training;\n",
    "* assessing the performance of our model via **cross-validation** instead of\n",
    "  a single train-test split.\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "First, let's charge the full adult census dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop the target from the data we will use to train our\n",
    "predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we select only the numerical columns, as seen in the previous\n",
    "notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    \"age\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"]\n",
    "\n",
    "data_numeric = data[numerical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can divide our dataset into a train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    data_numeric, target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting without preprocessing\n",
    "\n",
    "We will use the logistic regression classifier as in the previous notebook.\n",
    "This time, besides fitting the model, we will also compute the time needed\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not have issues with our model training: it converged in 59 iterations\n",
    "while we gave maximum number of iterations of 100. However, we can give an\n",
    "hint that this model could converge and train faster. In some case, we might\n",
    "even get a `ConvergenceWarning` using the above pattern. We will show such\n",
    "example by reducing the number of maximum iterations allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=50)\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model.n_iter_[0]} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the score is closed to the previous case because our algorithm\n",
    "is closed to the same solution. The warning suggested by scikit-learn\n",
    "provides two solutions to solve this issue:\n",
    "\n",
    "* increase `max_iter` which is indeed what happens in the former case. We let\n",
    "  the algorithm converge, it will take more time but we are sure to get an\n",
    "  optimal model;\n",
    "* standardize the data which is expected to improve convergence.\n",
    "\n",
    "We will investigate the second option.\n",
    "\n",
    "## Model fitting with preprocessing\n",
    "\n",
    "A range of preprocessing algorithms in scikit-learn allow us to transform\n",
    "the input data before training a model. In our case, we will standardize the\n",
    "data and then train a new logistic regression model on that new version of\n",
    "the dataset.\n",
    "\n",
    "Let's start by printing some statistics about the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset's features span across different ranges.\n",
    "However, we want to have normalized features because some algorithms make\n",
    "assumptions regarding their distribution, and being normalized is usually\n",
    "one of them.\n",
    "\n",
    "<div class=\"admonition tip alert alert-warning\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Tip</p>\n",
    "<p>Here are some reasons for scaling features:</p>\n",
    "<ul class=\"last simple\">\n",
    "<li>Models that rely on the distance between a pair of samples (e.g k-nearest\n",
    "neighbors) should be trained on normalized features to make each feature\n",
    "contribute approximately equally to the distance computations.</li>\n",
    "<li>Many models such as logistic regression use a numerical solver (based on\n",
    "gradient descent) to find their optimal parameters. This solver converges\n",
    "faster when the features are scaled.</li>\n",
    "<li>predictors using Euclidean distance (e.g k-nearest-neighbors) should have\n",
    "normalized features so that each one contributes equally to the distance\n",
    "computation;</li>\n",
    "<li>predictors using gradient-descent based algorithms (e.g logistic regression)\n",
    "to find optimal parameters work better and faster;</li>\n",
    "<li>predictors using regularization (e.g logistic regression) require normalized\n",
    "features to properly apply the weights.</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "Whether or not a machine learning model requires scaling the features depends\n",
    "on the model family. Linear models such as logistic regression generally\n",
    "benefit from scaling the features while other models such as decision trees do\n",
    "not need such preprocessing.\n",
    "\n",
    "We show how to apply such normalization using a scikit-learn transformer\n",
    "called `StandardScaler`. This transformer intend to transform feature such\n",
    "that each of them will all have a mean value and a unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_train_scaled = scaler.fit_transform(data_train)\n",
    "data_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled,\n",
    "                                 columns=data_train.columns)\n",
    "data_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily combine these sequential operations with a scikit-learn\n",
    "`Pipeline`, which chains together operations and can be used like any other\n",
    "classifier or regressor. The helper function `make_pipeline` will create a\n",
    "`Pipeline`: it takes as arguments the successive transformations to perform,\n",
    "followed by the classifier or regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "start = time.time()\n",
    "model.fit(data_train, target_train)\n",
    "elapsed_time = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.__class__.__name__\n",
    "score = model.score(data_test, target_test)\n",
    "print(f\"The accuracy using a {model_name} is {score:.3f} \"\n",
    "      f\"with a fitting time of {elapsed_time:.3f} seconds \"\n",
    "      f\"in {model[-1].n_iter_} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the training time and the number of iterations is much\n",
    "shorter while the predictive performance (accuracy) slightly improved.\n",
    "\n",
    "## Model evaluation using cross-validation\n",
    "\n",
    "In the previous example, we split the original data into a training set\n",
    "and a testing set.\n",
    "This strategy has several issues: in the setting where the amount of data\n",
    "is limited, the subset used to train or test will be small.\n",
    "Moreover, if the splitting was done in a random manner, we do not have\n",
    "information regarding the confidence of the results obtained.\n",
    "\n",
    "Instead, we can use cross-validation.\n",
    "Cross-validation consists of repeating\n",
    "this random splitting into training and testing sets and aggregating the\n",
    "model performance. By repeating the experiment, one can get an estimate of\n",
    "the variability of the model performance.\n",
    "\n",
    "The function `cross_val_score` allows for such experimental protocol by\n",
    "giving the model, the data and the target. Since there exists several\n",
    "cross-validation strategies, `cross_val_score` takes a parameter `cv` which\n",
    "defines the splitting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, data_numeric, target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The mean cross-validation accuracy is: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by computing the standard-deviation of the cross-validation scores\n",
    "we can get an idea of the uncertainty of our estimation of the predictive\n",
    "performance of the model: in the above results, only the first 2 decimals\n",
    "seem to be trustworthy. Using a single train / test split would not allow us\n",
    "to know anything about the level of uncertainty of the accuracy of the model.\n",
    "\n",
    "Setting `cv=5` created 5 distinct splits to get 5 variations for the training\n",
    "and testing sets. Each training set is used to fit one model which is then\n",
    "scored on the matching test set. This strategy is called K-fold\n",
    "cross-validation where `K` corresponds to the number of splits.\n",
    "\n",
    "The figure helps visualize how the dataset is partitioned into train and test\n",
    "samples at each iteration of the cross-validation procedure:\n",
    "\n",
    "![Cross-validation diagram](../figures/cross_validation_diagram.png)\n",
    "\n",
    "For each cross-validation split, the procedure trains a model on the\n",
    "concatenation of the red samples and evaluate the score of the model by using\n",
    "the blue samples. Cross-validation is therefore computationally intensive\n",
    "because it requires training several models instead of one.\n",
    "\n",
    "Note that the `cross_val_score` method above discards the 5 models that were\n",
    "trained on the different overlapping subset of the dataset. The goal of\n",
    "cross-validation is not to train a model, but rather to estimate\n",
    "approximately the generalization performance of a model that would have been\n",
    "trained to the full training set, along with an estimate of the variability\n",
    "(uncertainty on the generalization accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have:\n",
    "\n",
    "* seen the importance of **scaling numerical variables**;\n",
    "* used a **pipeline** to chain scaling and logistic regression training;\n",
    "* assessed the performance of our model via **cross-validation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
